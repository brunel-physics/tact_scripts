# Configuration file for ee channel classifier using multi-layer perceptron

# Directory containing input files for the classifier. Each file should contain
# Monte Carlo for a process, along with a separate file for the data.
input_dir: /scratch/data/TopPhysics/mvaDirs/inputs/2016/all/mz20mw20/

# Random number generation seed
seed: 777

# ROOT selection string specifying the cuts that should be made before
# classifier training takes place.
selection: >-
    chi2 < 40 &&
    Channel == 1

# channel - prefix for output file names
channel: ee

# List of processes which should be considered signal
signals:
    - tZq

# List of processes which should be considered background
backgrounds:
    - DYToLL_M10To50_aMCatNLO
    - DYToLL_M50_aMCatNLO
    - FakeEG
    - FakeMu
    - TbartChan
    - TbartW
    - THQ
    - TsChan
    - TT
    - TtChan
    - ttH
    - TTW
    - TtW
    - TTZ
    - TWZ
    - Wjets
    - WW
    - WWW
    - WWZ
    - WZ
    - WZZ
    - ZZ
    - ZZZ

# Name of process containing collision data
data_process: DataEG

# Directories plots, root files, and trained classifiers should be output into
plot_dir: plots/
root_dir: root/
mva_dir: mva/

# Fraction of data to be reserved in test sample
test_fraction: 0.33

# If true, the weights of the signal channels are linearly scaled so that the
# overall normalisation for both the signal and background channels is the same
equalise_signal: true

# How negative event weights should be treated
#   passthrough: negative weights are unaltered
#   abs: the absolute value of all negative weights is taken
#   reweight: The absolute value of all negative weights is taken. The original
#             normalisation for each process is then restored by linearly
#             scaling the resulting weights down. This will fail if any
#             processes have an overall negative weight.
negative_weight_treatment: passthrough

# Preprocessing
# List of preprocessors in the order they should be applied, accompanied by
# key-value pairs of keyword arguments under a config key
preprocessors:
    - preprocessor: robust_scaler
      config: {}

# Classifier selection
#   bdt_grad: Gradient Boosted Decision Tree (scikit-learn)
#   bdt_xgb: Gradient Boosted Decision Tree (XGBoost)
#   random_forest: Random Forest
#   mlp: Multi-Layer Perceptron (Keras)
#   load: load classfier specfied by classifer_path option
classifier: mlp

# MLP configuration
mlp:
    # YAML describing model, see Keras documentation for details
    model:
        backend: theano
        class_name: Sequential
        config:
        - class_name: Dense
          config:
            activation: tanh
            # activity_regularizer:
            #   class_name: L1L2
            #   config: {l1: 5e-6, l2: 1e-5}
            # kernel_regularizer:
            #   class_name: L1L2
            #   config: {l1: 5e-4, l2: 0}
            # bias_regularizer:
            #   class_name: L1L2
            #   config: {l1: 5e-4, l2: 0}
            units: 20
        - class_name: Dropout
          config:
              rate: 0.25
        - class_name: Dense
          config:
            activation: sigmoid
            units: 1

    # Keyword arguments to be passed to Keras' KerasClassifier() function
    model_params:
        epochs: 10000
        verbose: 1
        batch_size: 256

    # Keyword arguments to be sent to Keras' model.compile() function
    compile_params:
        loss: binary_crossentropy
        optimizer: adam
        metrics:
            - accuracy

    # Parameters passed to Keras' EarlyStopping()
    early_stopping_params:
        monitor: loss
        min_delta: 0
        patience: 5
        verbose: 1
        mode: auto

    # Parameters passed to Keras' ReduceLROnPlateau()
    lr_reduction_params:
        monitor: loss
        factor: 0.2
        patience: 2
        verbose: 1

# Options governing the root file output
root_out:
    # Whether output should be in the format for combine (true) or THETA (false)
    combine: true

    # What form the (pseudo)-data in the files should take
    # empty: Empty histograms
    # poisson: Sum the Monte Carlo histograms, and perform a Poisson jump on
    #          each bin
    # real: Use the real data
    data: empty

    # The strategy used to bin the MVA response in the resulting root files
    #   equal: specified number of equal-width bins in the (0, 1) range
    #          (default).
    #   quantile: specified number of equally-populated bins, achieved by
    #             placing bin edges at quantiles. Bin population does not take
    #             event weight into account.
    #   recursive_median: response is recursively bisected at the median
    #   recursive_kmeans: response is recursively split into two clusters using
    #                     the k-means (Jenks) algorithm
    strategy: recursive_median

    # Set the number of bins for the equal or quantile binning stategies
    bins: 20

    # The recursive binning strategies will stop splitting once these limits
    # are reached
    min_signal_events: 0
    min_background_events: 1
    max_signal_error: 0.3
    max_background_error: 0.3

features:
    - bTagDisc
    # - fourthJetEta
    # - fourthJetPhi
    - fourthJetPt
    # - fourthJetbTag
    # - jetHt
    - jetMass
    # - jetMass3
    # - jjdelPhi
    - jjdelR
    # - leadJetEta
    # - leadJetPhi
    - leadJetPt
    # - leadJetbTag
    # - lep1D0
    # - lep1Eta
    # - lep1Phi
    # - lep1Pt
    # - lep1RelIso
    # - lep2D0
    # - lep2Eta
    # - lep2Phi
    # - lep2Pt
    # - lep2RelIso
    # - lepEta
    - lepHt
    # - mTW
    - met
    # - nBjets
    # - nJets
    # - secJetEta
    # - secJetPhi
    - secJetPt
    # - secJetbTag
    # - thirdJetEta
    # - thirdJetPhi
    - thirdJetPt
    # - thirdJetbTag
    # - topEta
    - topMass
    # - topPhi
    # - topPt
    # - totEta
    # - totHt
    - totHtOverPt
    # - totPt
    # - totPt2Jet
    # - totPtVec
    # - totVecM
    # - w1TopDelPhi
    # - w1TopDelR
    # - w2TopDelPhi
    # - w2TopDelR
    # - wPairEta
    - wPairMass
    # - wPairPhi
    # - wPairPt
    # - wQuark1Eta
    # - wQuark1Phi
    # - wQuark1Pt
    # - wQuark2Eta
    # - wQuark2Phi
    # - wQuark2Pt
    # - wQuarkHt
    # - wTopDelPhi
    # - wTopDelR
    # - wwdelPhi
    # - wwdelR
    # - wzdelPhi
    # - wzdelR
    # - zEta
    - zLepdelPhi
    # - zLepdelR
    - zMass
    # - zPhi
    # - zPt
    # - zQuark1DelPhi
    # - zQuark1DelR
    # - zQuark2DelPhi
    # - zQuark2DelR
    # - zTopDelPhi
    # - zTopDelR
    # - zjminPhi
    - zjminR
    # - zl1Quark1DelPhi
    # - zl1Quark1DelR
    # - zl1Quark2DelPhi
    # - zl1Quark2DelR
    # - zl1TopDelPhi
    # - zl1TopDelR
    # - zl2Quark1DelPhi
    # - zl2Quark1DelR
    # - zl2Quark2DelPhi
    # - zl2Quark2DelR
    # - zl2TopDelPhi
    # - zl2TopDelR
    # - zlb1DelPhi
    # - zlb1DelR
    # - zlb2DelPhi
    # - zlb2DelR
