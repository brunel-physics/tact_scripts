# Configuration file for ee channel classifier using multi-layer perceptron

# Directory containing input files for the classifier. Each file should contain
# Monte Carlo for a process, along with a separate file for the data.
input_dir: /scratch/data/TopPhysics/mvaDirs/inputs/2016/all/mz20mw20/

# Random number generation seed
seed: 777

# ROOT selection string specifying the cuts that should be made before
# classifier training takes place.
selection: >-
    Channel == 1.0

# channel - prefix for output file names
channel: ee

# List of processes which should be considered signal
signals:
    - TZQ

# List of processes which should be considered background
backgrounds:
    - "TTHbb"
    - "TTHnonbb"
    - "WWW"
    - "WWZ"
    - "WZZ"
    - "ZZZ"
    - "WW1l1nu2q"
    - "WW2l2nu"
    - "ZZ4l"
    - "ZZ2l2nu"
    - "ZZ2l2q"
    - "WZjets"
    - "WZ2l2q"
    - "WZ1l1nu2q"
    - "TsChan"
    - "TtChan"
    - "TbartChan"
    - "TW"
    - "TbarW"
    - "TZQ"
    - "THQ"
    - "TTWlnu"
    - "TTW2q"
    - "TTZ2l2nu"
    - "TTZ2q"
    - "TT"
    - "TWZ"
    - "Wjets"
    - "DYJetsLLPt0To50"
    - "DYJetsLLPt50To100"
    - "DYJetsLLPt100To250"
    - "DYJetsLLPt250To400"
    - "DYJetsLLPt400To650"
    - "DYJetsLLPt650ToInf"

# Name of process containing collision data
data_process: DataEG

# Directories plots, root files, and trained classifiers should be output into
plot_dir: plots/
root_dir: root/
mva_dir: mva/

# Fraction of data to be reserved in test sample
test_fraction: 0.2

# If true, the weights of the signal channels are linearly scaled so that the
# overall normalisation for both the signal and background channels is the same
equalise_signal: true

# How negative event weights should be treated
#   passthrough: negative weights are unaltered
#   abs: the absolute value of all negative weights is taken
#   reweight: The absolute value of all negative weights is taken. The original
#             normalisation for each process is then restored by linearly
#             scaling the resulting weights down. This will fail if any
#             processes have an overall negative weight.
#   zero: negative weights are set to 0
negative_weight_treatment: passthrough

# Preprocessing
# List of preprocessors in the order they should be applied, accompanied by
# key-value pairs of keyword arguments under a config key
preprocessors:
    - preprocessor: robust_scaler
      config: {}

# Classifier selection
#   bdt_grad: Gradient Boosted Decision Tree (scikit-learn)
#   bdt_xgb: Gradient Boosted Decision Tree (XGBoost)
#   random_forest: Random Forest
#   mlp: Multi-Layer Perceptron (Keras)
#   load: load classfier specfied by classifer_path option
classifier: mlp

# MLP configuration
mlp:
    # YAML describing model, see Keras documentation for details
    model:
        class_name: Sequential
        config:
        - class_name: Dense
          config:
            activation: "relu"
            kernel_initializer: "glorot_normal"
            bias_initializer: "ones"
            kernel_regularizer:
              class_name: L1L2
              config: {l1: 1e-5, l2: 1e-4}
            units: 32
        - class_name: Dropout
          config:
              rate: 0.3
        - class_name: Dense
          config:
            activation: "relu"
            kernel_initializer: "glorot_normal"
            bias_initializer: "ones"
            kernel_regularizer:
              class_name: L1L2
              config: {l1: 1e-4, l2: 1e-3}
            units: 32
        - class_name: Dropout
          config:
              rate: 0.3
        - class_name: Dense
          config:
            activation: sigmoid
            kernel_initializer: "glorot_normal"
            bias_initializer: "ones"
            units: 1

    # Keyword arguments to be passed to Keras' KerasClassifier() function
    model_params:
        epochs: 10000
        verbose: 1
        batch_size: 256

    # Keyword arguments to be sent to Keras' model.compile() function
    compile_params:
        loss: binary_crossentropy
        optimizer: adam
        metrics:
            - accuracy

    # Parameters passed to Keras' EarlyStopping()
    early_stopping_params:
        monitor: loss
        min_delta: 0
        patience: 50
        verbose: 1
        mode: auto

    # Parameters passed to Keras' ReduceLROnPlateau()
    lr_reduction_params:
        monitor: loss
        factor: 0.1
        patience: 20
        verbose: 1

# Options governing the root file output
root_out:
    # Whether output should be in the format for combine (true) or THETA (false)
    combine: true

    # What form the (pseudo)-data in the files should take
    # empty: Empty histograms
    # poisson: Sum the Monte Carlo histograms, and perform a Poisson jump on
    #          each bin
    # real: Use the real data
    data: empty

    # The strategy used to bin the MVA response in the resulting root files
    #   equal: specified number of equal-width bins in the (0, 1) range
    #          (default).
    #   quantile: specified number of equally-populated bins, achieved by
    #             placing bin edges at quantiles. Bin population does not take
    #             event weight into account.
    #   recursive_median: response is recursively bisected at the median
    #   recursive_kmeans: response is recursively split into two clusters using
    #                     the k-means (Jenks) algorithm
    strategy: recursive_median

    # Set the number of bins for the equal or quantile binning stategies
    bins: 20

    # The recursive binning strategies will stop splitting once these limits
    # are reached
    min_signal_events: 0
    min_background_events: 1
    max_signal_error: 0.3
    max_background_error: 0.3

features:
    # - bTagDisc
    - fourthJetEta
    - fourthJetPhi
    - fourthJetPt
    - fourthJetbTag
    # - jetHt
    # - jetMass
    # - jetMass3
    # - jjdelPhi
    # - jjdelR
    - leadJetEta
    - leadJetPhi
    - leadJetPt
    # - leadJetbTag
    # - lep1D0
    - lep1Eta
    - lep1Phi
    - lep1Pt
    # - lep1RelIso
    # - lep2D0
    # - lep2Eta
    - lep2Phi
    - lep2Pt
    # - lep2RelIso
    - lepEta
    # - lepHt
    # - mTW
    # - met
    # - nBjets
    # - nJets
    - secJetEta
    - secJetPhi
    - secJetPt
    - secJetbTag
    - thirdJetEta
    - thirdJetPhi
    - thirdJetPt
    - thirdJetbTag
    # - topEta
    # - topMass
    # - topPhi
    # - topPt
    # - totEta
    # - totHt
    # - totHtOverPt
    # - totPt
    # - totPt2Jet
    # - totPtVec
    # - totVecM
    # - w1TopDelPhi
    # - w1TopDelR
    # - w2TopDelPhi
    # - w2TopDelR
    # - wPairEta
    # - wPairMass
    # - wPairPhi
    # - wPairPt
    # - wQuark1Eta
    # - wQuark1Phi
    # - wQuark1Pt
    # - wQuark2Eta
    # - wQuark2Phi
    # - wQuark2Pt
    # - wQuarkHt
    # - wTopDelPhi
    # - wTopDelR
    # - wwdelPhi
    # - wwdelR
    # - wzdelPhi
    # - wzdelR
    # - zEta
    # - zLepdelPhi
    # - zLepdelR
    # - zMass
    # - zPhi
    # - zPt
    # - zQuark1DelPhi
    # - zQuark1DelR
    # - zQuark2DelPhi
    # - zQuark2DelR
    # - zTopDelPhi
    # - zTopDelR
    # - zjminPhi
    # - zjminR
    # - zl1Quark1DelPhi
    # - zl1Quark1DelR
    # - zl1Quark2DelPhi
    # - zl1Quark2DelR
    # - zl1TopDelPhi
    # - zl1TopDelR
    # - zl2Quark1DelPhi
    # - zl2Quark1DelR
    # - zl2Quark2DelPhi
    # - zl2Quark2DelR
    # - zl2TopDelPhi
    # - zl2TopDelR
    # - zlb1DelPhi
    # - zlb1DelR
    # - zlb2DelPhi
    # - zlb2DelR
    - fifthJetEta
    - fifthJetPhi
    - fifthJetPt
    - fifthJetbTag
    - sixthJetEta
    - sixthJetPhi
    - sixthJetPt
    - sixthJetbTag
