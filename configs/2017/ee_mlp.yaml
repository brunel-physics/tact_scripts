# Configuration file for ee channel classifier using multi-layer perceptron

# Directory containing input files for the classifier. Each file should contain
# Monte Carlo for a process, along with a separate file for the data.
input_dir: /scratch/data/TopPhysics/mvaDirs/inputs/2017/all/mz20mw20/

# Random number generation seed
seed: 777

# ROOT selection string specifying the cuts that should be made before
# classifier training takes place.
selection: >-
    Channel == 1.0

# channel - prefix for output file names
channel: ee

# List of processes which should be considered signal
signals:
    - "TZQ"

# List of processes which should be considered background
backgrounds:
    - "TTHbb"
    - "TTHnonbb"
    # - "WWW"
    - "WWZ"
    - "WZZ"
    - "ZZZ"
    - "WW1l1nu2q"
    - "WW2l2nu"
    - "ZZ4l"
    - "ZZ2l2nu"
    - "ZZ2l2q"
    - "WZ3l3nu"
    - "WZ2l2q"
    - "WZ1l1nu2q"
    - "TsChan"
    - "TtChan"
    - "TbartChan"
    - "TW"
    - "TbarW"
    - "THQ"
    - "TTWlnu"
    - "TTW2q"
    - "TTZ2l2nu"
    - "TTZ2q"
    - "TT2l2v"
    - "TTjets"
    - "TT1l1v2q"
    - "TTG"
    - "TWZ"
    - "Wjets"
    - "ZG2l1g"
    - "WG11nu1g"
    - "DYJetsToLLM10to50"
    - "DYJetsToLLM50"
    - "FakeEG"

# Name of process containing collision data
data_process: DataEG

# Directories plots, root files, and trained classifiers should be output into
plot_dir: plots/
root_dir: root/
mva_dir: mva/

# Fraction of data to be reserved in test sample
test_fraction: 0.2

# If true, the weights of the signal channels are linearly scaled so that the
# overall normalisation for both the signal and background channels is the same
equalise_signal: true

# How negative event weights should be treated
#   passthrough: negative weights are unaltered
#   abs: the absolute value of all negative weights is taken
#   reweight: The absolute value of all negative weights is taken. The original
#             normalisation for each process is then restored by linearly
#             scaling the resulting weights down. This will fail if any
#             processes have an overall negative weight.
#   zero: negative weights are set to 0
negative_weight_treatment: passthrough

# Preprocessing
# List of preprocessors in the order they should be applied, accompanied by
# key-value pairs of keyword arguments under a config key
preprocessors:
    - preprocessor: standard_scaler
      config: {}

# Classifier selection
#   bdt_grad: Gradient Boosted Decision Tree (scikit-learn)
#   bdt_xgb: Gradient Boosted Decision Tree (XGBoost)
#   random_forest: Random Forest
#   mlp: Multi-Layer Perceptron (Keras)
#   load: load classfier specfied by classifer_path option
classifier: mlp

# MLP configuration
mlp:
    # YAML describing model, see Keras documentation for details
    model:
        class_name: Sequential
        config:
        - class_name: Dense
          config:
            activation: "tanh"
            kernel_initializer: "glorot_normal"
            bias_initializer: "ones"
            kernel_regularizer:
              class_name: L1L2
              config: {l1: 5e-4, l2: 2e-2}
            units: 16
        - class_name: Dropout
          config:
              rate: 0.25
        # - class_name: Dense
        #   config:
        #     activation: "relu"
        #     kernel_initializer: "glorot_normal"
        #     bias_initializer: "ones"
        #     kernel_regularizer:
        #       class_name: L1L2
        #       config: {l1: 1e-4, l2: 1e-3}
        #     units: 64
        # - class_name: Dropout
        #   config:
        #       rate: 0.3333333333333333
        - class_name: Dense
          config:
            activation: sigmoid
            kernel_initializer: "glorot_normal"
            bias_initializer: "ones"
            units: 1

    # Keyword arguments to be passed to Keras' KerasClassifier() function
    model_params:
        epochs: 10000
        verbose: 1
        batch_size: 4096

    # Keyword arguments to be sent to Keras' model.compile() function
    compile_params:
        loss: binary_crossentropy
        optimizer: adam
        metrics:
            - accuracy

    # Parameters passed to Keras' EarlyStopping()
    early_stopping_params:
        monitor: val_loss
        min_delta: 0
        patience: 15
        verbose: 1
        mode: auto

    # Parameters passed to Keras' ReduceLROnPlateau()
    lr_reduction_params:
        monitor: val_loss
        factor: 0.1
        patience: 9999
        verbose: 1

# Options governing the root file output
root_out:
    # Whether output should be in the format for combine (true) or THETA (false)
    combine: true

    # What form the (pseudo)-data in the files should take
    # empty: Empty histograms
    # poisson: Sum the Monte Carlo histograms, and perform a Poisson jump on
    #          each bin
    # real: Use the real data
    data: real

    # The strategy used to bin the MVA response in the resulting root files
    #   equal: specified number of equal-width bins in the (0, 1) range
    #          (default).
    #   quantile: specified number of equally-populated bins, achieved by
    #             placing bin edges at quantiles. Bin population does not take
    #             event weight into account.
    #   recursive_median: response is recursively bisected at the median
    #   recursive_kmeans: response is recursively split into two clusters using
    #                     the k-means (Jenks) algorithm
    strategy: recursive_median

    # Set the number of bins for the equal or quantile binning stategies
    bins: 20

    # The recursive binning strategies will stop splitting once these limits
    # are reached
    min_signal_events: 0
    min_background_events: 1
    max_signal_error: 0.3
    max_background_error: 0.3

    # Suffix to be added to bin and systematic names
    suffix: "_2017"

features:
    # - "bEta"
    # - "bPhi"
    # - "bPt"
    - "bbTag"
    # - "chi2"
    - "j1Eta"
    - "j1Phi"
    - "j1Pt"
    # - "j1bDelR"
    - "j1bTag"
    # - "j1j2DelR"
    # - "j1j3DelR"
    # - "j1j4DelR"
    # - "j1l1DelR"
    # - "j1l2DelR"
    # - "j1tDelR"
    # - "j1wDelR"
    # - "j1wj1DelR"
    # - "j1wj2DelR"
    # - "j1zDelR"
    - "j2Eta"
    - "j2Phi"
    - "j2Pt"
    # - "j2bDelR"
    - "j2bTag"
    # - "j2j3DelR"
    # - "j2j4DelR"
    # - "j2l1DelR"
    # - "j2l2DelR"
    # - "j2tDelR"
    # - "j2wDelR"
    # - "j2wj1DelR"
    # - "j2wj2DelR"
    # - "j2zDelR"
    - "j3Eta"
    - "j3Phi"
    - "j3Pt"
    # - "j3bDelR"
    - "j3bTag"
    # - "j3j4DelR"
    # - "j3l1DelR"
    # - "j3l2DelR"
    # - "j3tDelR"
    # - "j3wDelR"
    # - "j3wj1DelR"
    # - "j3wj2DelR"
    # - "j3zDelR"
    - "j4Eta"
    - "j4Phi"
    - "j4Pt"
    # - "j4bDelR"
    - "j4bTag"
    # - "j4l1DelR"
    # - "j4l2DelR"
    # - "j4tDelR"
    # - "j4wDelR"
    # - "j4wj1DelR"
    # - "j4wj2DelR"
    # - "j4zDelR"
    - "jetMass"
    # - "jetMass3"
    # - "jetMt"
    # - "jetPt"
    # - "l1D0"
    - "l1Eta"
    - "l1Phi"
    - "l1Pt"
    - "l1RelIso"
    # - "l1bDelR"
    # - "l1tDelR"
    # - "l1wj1DelR"
    # - "l1wj2DelR"
    # - "l2D0"
    - "l2Eta"
    - "l2Phi"
    - "l2Pt"
    - "l2RelIso"
    # - "l2bDelR"
    # - "l2tDelR"
    # - "l2wj1DelR"
    # - "l2wj2DelR"
    - "met"
    # - "nBjets"
    # - "nJets"
    # - "tEta"
    - "tMass"
    # - "tMt"
    # - "tPhi"
    # - "tPt"
    # - "tbDelR"
    # - "totMass"
    # - "totMt"
    # - "totPt"
    # - "wEta"
    # - "wMass"
    # - "wMt"
    # - "wPhi"
    # - "wPt"
    # - "wbDelR"
    # - "wj1DelR"
    # - "wj1Eta"
    # - "wj1Phi"
    # - "wj1Pt"
    # - "wj1bDelR"
    # - "wj1tDelR"
    # - "wj2DelR"
    # - "wj2Eta"
    # - "wj2Phi"
    # - "wj2Pt"
    # - "wj2bDelR"
    # - "wj2tDelR"
    # - "wtDelR"
    # - "wwDelR"
    # - "wzDelR"
    # - "zEta"
    - "zMass"
    # - "zMt"
    # - "zPhi"
    # - "zPt"
    # - "zbDelR"
    - "zjMaxR"
    # - "zjMinR"
    # - "ztDelR"
    # - "zwj1DelR"
    # - "zwj2DelR"
    # - "zzDelR"
